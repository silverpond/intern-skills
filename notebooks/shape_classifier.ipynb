{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Train A Shape Classifier Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "train_data_root = \"../datasets/train\"\n",
    "test_data_root = \"../datasets/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['circle', 'diamond', 'triangle']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations (including resizing and normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale (black and white images)\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64 pixels\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images (mean=0.5, std=0.5 for grayscale)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root=train_data_root, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_data_root, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check class names (optional)\n",
    "print(f'Classes: {train_dataset.classes}')\n",
    "\n",
    "# 2. Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) # change in coming channel to 16\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 3 classes: circle, triangle, rectangle\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # First Conv Layer\n",
    "        x = F.max_pool2d(x, 2)      # Max Pooling\n",
    "        x = F.relu(self.conv2(x))   # Second Conv Layer\n",
    "        x = F.max_pool2d(x, 2)      # Max Pooling\n",
    "        x = x.view(x.size(0), -1)   # Flatten\n",
    "        x = F.relu(self.fc1(x))     # Fully Connected Layer 1\n",
    "        x = self.fc2(x)             # Fully Connected Layer 2 (output)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total # change the accuracy calculation\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return total, predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.0989, Accuracy: 38.00%\n",
      "Epoch [2/15], Loss: 1.0185, Accuracy: 47.86%\n",
      "Epoch [3/15], Loss: 0.8985, Accuracy: 57.29%\n",
      "Epoch [4/15], Loss: 0.7515, Accuracy: 69.00%\n",
      "Epoch [5/15], Loss: 0.5426, Accuracy: 80.86%\n",
      "Epoch [6/15], Loss: 0.3537, Accuracy: 88.86%\n",
      "Epoch [7/15], Loss: 0.1992, Accuracy: 95.71%\n",
      "Epoch [8/15], Loss: 0.1195, Accuracy: 97.57%\n",
      "Epoch [9/15], Loss: 0.0635, Accuracy: 99.43%\n",
      "Epoch [10/15], Loss: 0.0398, Accuracy: 99.57%\n",
      "Epoch [11/15], Loss: 0.0284, Accuracy: 99.71%\n",
      "Epoch [12/15], Loss: 0.0136, Accuracy: 100.00%\n",
      "Epoch [13/15], Loss: 0.0084, Accuracy: 100.00%\n",
      "Epoch [14/15], Loss: 0.0055, Accuracy: 100.00%\n",
      "Epoch [15/15], Loss: 0.0042, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "all_label, predicted_label = train_model(model, train_loader, criterion, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def test(model, test_loader):\n",
    "    \"\"\"Print the Precision, Recall and F1-score for the trained model\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Print results\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-score: {f1:.4f}')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Show Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_prediction(model, image):\n",
    "    \"\"\"Pass the image to the model and overlay the predicted shape and confidence on the input\n",
    "    image and display it\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Define transformations (including resizing and normalization)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert to grayscale (black and white images)\n",
    "        transforms.Resize((64, 64)),  # Resize images to 64x64 pixels\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize the images (mean=0.5, std=0.5 for grayscale)\n",
    "    ])\n",
    "\n",
    "    # Apply transformation\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        output = model(image_tensor)  \n",
    "        probabilities = torch.softmax(output, dim=1) \n",
    "        confidence, predicted_class = torch.max(probabilities, 1)  \n",
    "\n",
    "    predicted_label = test_dataset.classes[predicted_class.item()]\n",
    "    confidence = confidence.item()\n",
    "\n",
    "    # Convert back to displayable format \n",
    "    image_to_display = image_tensor.squeeze(0).cpu().numpy()  # Remove batch dim, convert to NumPy\n",
    "    image_to_display = (image_to_display * 0.5) + 0.5  # Denormalize\n",
    "\n",
    "    # Display image with prediction\n",
    "    plt.imshow(image_to_display.squeeze(), cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted_label} ({confidence * 100:.2f}%)')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"../datasets/test/circle/circle_103.png\")\n",
    "\n",
    "show_prediction(model, image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern-skills",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
